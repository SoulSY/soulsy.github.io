<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>인공지능 on SamTech</title>
    <link>https://soulsy.github.io/ko/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/</link>
    <description>Recent content in 인공지능 on SamTech</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Sun, 16 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://soulsy.github.io/ko/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>트랜스포머 모델</title>
      <link>https://soulsy.github.io/ko/knowledge/tensorflow/transformer_models/</link>
      <pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://soulsy.github.io/ko/knowledge/tensorflow/transformer_models/</guid>
      <description>트랜스포머 모델 (Transformer Models) 트랜스포머 모델은 연속 데이터를 처리하는 새로운 방법론을 도입함으로써 자연어 처리(NLP) 분야에 혁명을 일으켰습니다. 2017년 Vaswani 등이 &amp;ldquo;Attention is All You Need&amp;rdquo; 라는 논문에서 개발한 이 구조는 자기 주목(self-attention) 메커니즘에 크게 의존하여 문장의 단어 중요도를 가중치로 부여하고, 이를 통해 모델이 문맥을 더욱 잘 이해하게 합니다.
트랜스포머 모델의 특징 RNNs와 LSTMs 같은 이전의 연속 데이터 처리 모델들이 데이터를 순차적으로 처리하는 반면, 트랜스포머는 모든 시퀀스 입력을 동시에 처리합니다. 이로 인해 트랜스포머는 연속 데이터에서 장거리 의존성을 더 효과적으로 처리할 수 있습니다.</description>
    </item>
    
  </channel>
</rss>